# AlgoRhythm ğŸ“»

[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v2.json)](https://github.com/charliermarsh/ruff)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)
[![security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](https://mypy-lang.org/)

## ğŸ¯ Description

# ğŸµ AlgoRhythm

**AlgoRhythm** is a music and album recommendation system based on the musical preferences of real or synthetic users. It uses Spotify's public API to extract real-time data and generate personalized suggestions. Developed as the final project for the **Data Science in Production** course.

## ğŸš€ What does this project do?

-   Builds user music profiles based on:
    -   Favorite songs and albums (real from Spotify or simulated via surveys/synthetic data)
    -   Ratings or popularity scores
-   Extracts dynamic data from Spotify using its API:
    -   Information about tracks, albums, and artists
    -   Popular playlists such as the global or genre-specific Top 50
-   Applies a content-based recommendation system:
    -   Compares musical attributes (genre, popularity, artists) between favorite songs and songs in the charts
    -   Uses clustering and similarity to suggest new music
    -   Filters out songs the user already listens to
-   Delivers personalized recommendations per user
-   Includes visualizations to interpret musical profiles and recommendations

The solution is built using a professional data engineering approach, covering the entire pipeline: from data acquisition and processing to training, evaluating, and deploying recommendation models.

## âœ¨ Features and Tools

Tools used in this project:

| Feature                         | Tool                     | Justification                           |
| ------------------------------- | ------------------------ | --------------------------------------- |
| Dependency management           | [UV]                     | Fast and reproducible environments      |
| Configuration management        | [Hydra]                  | Flexible parameter handling             |
| Code quality (linting, imports) | [Ruff]                   | Clean and consistent code               |
| Static type checking            | [Mypy]                   | Robustness and early error detection    |
| Code security                   | [Bandit]                 | Vulnerability detection                 |
| Pre-commit validations          | [Pre-commit]             | Maintain code quality standards         |
| Unit testing                    | [Pytest]                 | System behavior validation              |
| Test coverage tracking          | [coverage.py] [Codecov]  | Monitor test completeness               |
| Project templating              | [Cruft] / [Cookiecutter] | Professional data science template      |
| Professional data structure     | [Data structure]         | Clear and scalable project organization |

## ğŸš€ Environment Setup

1. Initialize the Git repository:

    ```bash
    make init_git
    ```

1. Set up the environment:

    ```bash
    make init_env
    ```

1. Install libraries for data science and machine learning:

    ```bash
    make install_data_libs
    ```

## Install dependencies

After init the environment to install a new package, run:

```bash
uv add <package-name>
```

Example to install [plotly](https://plotly.com/python/) in dev group:

```bash
uv add --group dev plotly
```

## ğŸ—ƒï¸ Project structure

-   [Data structure]
-   [Pipelines based on Feature/Training/Inference Pipelines](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines)

```bash
.
â”œâ”€â”€ codecov.yml                         # configuration for codecov
â”œâ”€â”€ .code_quality
â”‚Â Â  â”œâ”€â”€ mypy.ini                        # mypy configuration
â”‚Â Â  â””â”€â”€ ruff.toml                       # ruff configuration
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ 01_raw                          # raw immutable data
â”‚Â Â  â”œâ”€â”€ 02_intermediate                 # typed data
â”‚Â Â  â”œâ”€â”€ 03_primary                      # domain model data
â”‚Â Â  â”œâ”€â”€ 04_feature                      # model features
â”‚Â Â  â”œâ”€â”€ 05_model_input                  # often called 'master tables'
â”‚Â Â  â”œâ”€â”€ 06_models                       # serialized models
â”‚Â Â  â”œâ”€â”€ 07_model_output                 # data generated by model runs
â”‚Â Â  â”œâ”€â”€ 08_reporting                    # reports, results, etc
â”‚Â Â  â””â”€â”€ README.md                       # description of the data structure
â”œâ”€â”€ docs                                # documentation for your project
â”œâ”€â”€ .editorconfig                       # editor configuration
â”œâ”€â”€ .github                             # github configuration
â”‚Â Â  â”œâ”€â”€ dependabot.md                   # github action to update dependencies
â”‚Â Â  â”œâ”€â”€ pull_request_template.md        # template for pull requests
â”‚Â Â  â””â”€â”€ workflows                       # github actions workflows
â”‚Â Â      â”œâ”€â”€ ci.yml                      # run continuous integration (tests, pre-commit, etc.)
â”‚Â Â      â”œâ”€â”€ dependency_review.yml       # review dependencies
â”‚Â Â      â”œâ”€â”€ docs.yml                    # build documentation (mkdocs)
â”‚Â Â      â””â”€â”€ pre-commit_autoupdate.yml   # update pre-commit hooks
â”œâ”€â”€ .gitignore                          # files to ignore in git
â”œâ”€â”€ Makefile                            # useful commands to setup environment, run tests, etc.
â”œâ”€â”€ models                              # store final models
â”œâ”€â”€ notebooks
â”‚Â Â  â”œâ”€â”€ 1-data                          # data extraction and cleaning
â”‚Â Â  â”œâ”€â”€ 2-exploration                   # exploratory data analysis (EDA)
â”‚Â Â  â”œâ”€â”€ 3-analysis                      # Statistical analysis, hypothesis testing.
â”‚Â Â  â”œâ”€â”€ 4-feat_eng                      # feature engineering (creation, selection, and transformation.)
â”‚Â Â  â”œâ”€â”€ 5-models                        # model training, evaluation, and hyperparameter tuning.
â”‚Â Â  â”œâ”€â”€ 6-interpretation                # model interpretation
â”‚Â Â  â”œâ”€â”€ 7-deploy                        # model packaging, deployment strategies.
â”‚Â Â  â”œâ”€â”€ 8-reports                       # story telling, summaries and analysis conclusions.
â”‚Â Â  â”œâ”€â”€ notebook_template.ipynb         # template for notebooks
â”‚Â Â  â””â”€â”€ README.md                       # information about the notebooks
â”œâ”€â”€ .pre-commit-config.yaml             # configuration for pre-commit hooks
â”œâ”€â”€ pyproject.toml                      # dependencies for the python project
â”œâ”€â”€ README.md                           # description of your project
â”œâ”€â”€ src                                 # source code for use in this project
â”‚   â”œâ”€â”€ README.md                       # description of src structure
â”‚   â”œâ”€â”€ tmp_mock.py                     # example python file
â”‚   â”œâ”€â”€ data                            # data extraction, validation, processing, transformation
â”‚   â”œâ”€â”€ model                           # model training, evaluation, validation, export
â”‚   â”œâ”€â”€ inference                       # model prediction, serving, monitoring
â”‚   â””â”€â”€ pipelines                       # orchestration of pipelines
â”‚       â”œâ”€â”€ feature_pipeline            # transforms raw data into features and labels
â”‚       â”œâ”€â”€ training_pipeline           # transforms features and labels into a model
â”‚       â””â”€â”€ inference_pipeline          # takes features and a trained model for predictions
â”œâ”€â”€ tests                               # test code for your project
â”‚   â”œâ”€â”€ test_mock.py                    # example test file
â”‚   â”œâ”€â”€ data                            # tests for data module
â”‚   â”œâ”€â”€ model                           # tests for model module
â”‚   â”œâ”€â”€ inference                       # tests for inference module
â”‚   â””â”€â”€ pipelines                       # tests for pipelines module
â””â”€â”€ .vscode                             # vscode configuration
    â”œâ”€â”€ extensions.json                 # list of recommended extensions
    â”œâ”€â”€ launch.json                     # vscode launch configuration
    â””â”€â”€ settings.json                   # vscode settings
```

## ğŸ‘¥ Authors

-   SimÃ³n Correa MarÃ­n
-   Luis Felipe Ospina Giraldo

## Credits

This project was generated from [@JoseRZapata]'s [data science project template] template.

---

[@JoseRZapata]: https://github.com/JoseRZapata
[bandit]: https://github.com/PyCQA/bandit
[codecov]: https://codecov.io/
[Cookiecutter]: https://cookiecutter.readthedocs.io/en/stable/
[coverage.py]: https://coverage.readthedocs.io/
[Cruft]: https://cruft.github.io/cruft/
[data science project template]: https://github.com/JoseRZapata/data-science-project-template
[Data structure]: https://github.com/JoseRZapata/data-science-project-template/blob/main/music-recommendation-system/data/README.md
[hydra]: https://hydra.cc/
[Mypy]: http://mypy-lang.org/
[pre-commit]: https://pre-commit.com/
[Pytest]: https://docs.pytest.org/en/latest/
[Ruff]: https://docs.astral.sh/ruff/
[UV]: https://docs.astral.sh/uv/
